#!/usr/bin/env python

""" Cleans SSTables on GS """

import argparse
import json
import logging
import os
import socket
import sys
import StringIO
import subprocess
import time
import tempfile

now = None

def days_ago(tstamp):
    global now
    if now is None:
        now = time.time()
    delta = now - tstamp
    days = delta / 86400.0
    return days

def clean_backups(args, log):

    if args.debug:
        log.setLevel(logging.DEBUG)

    if not args.name:
        args.name = socket.getfqdn() + ":"

    try:
        log.debug("gsutil ls -lR gs://%s/%s%s" % (args.bucket,args.name,args.path))
        tmpfile = tempfile.NamedTemporaryFile(delete=False)
        process = subprocess.Popen("gsutil ls -lR gs://%s/%s%s | grep -v TOTAL" % (args.bucket, args.name, args.path), shell=True, stdout=tmpfile)
        process.communicate()[0]
        if process.returncode != 0:
            raise Exception("%s/%s%s" % (args.bucket, args.name, args.path))
        else:
            tmpfile.close()
            log.info("Connected to GS, getting keys ...")
            keys = open(tmpfile.name,"r")
            key_list=[]
            directory = False
            if keys:
                for key in keys:
                    key = key.strip()
                    if key.endswith("/:"):
                        log.debug("Directory %s" % (key))
                        directory = True
                    else:
                        if len(key) > 0:
                            log.debug("File %s" % (key))
                        directory = False

                    if not directory and (len(key) > 0):
                        last_modified = key.split(" ")[2]
                        file_name = key.split(" ")[4]
                        key_list.append((file_name, time.mktime(time.strptime(last_modified, "%Y-%m-%dT%H:%M:%SZ"))))

    except Exception as e:
        log.error('Problem getting keys from GS bucket: %s', e)
        if keys:
            keys.close()
        if tmpfile:
            tmpfile.close()
            os.unlink(tmpfile.name)
        sys.exit(1)

    if keys:
        keys.close()
        os.unlink(tmpfile.name)
        
    json_keys = []
    to_delete = set() # we'll remove from this list
    for k in sorted(
        key_list,
        key=lambda x: x[1],
        reverse=True, # most recent first
    ):
        to_delete.add(k[0])
        if k[0].endswith('-listdir.json'):
            json_keys.append(k)

    log.info("%s keys total", len(to_delete))
    log.debug("%s json listdir keys", len(json_keys))

    is_old = False  # used in testing for existence of file on server
    for jkey in json_keys:
        log.debug("key dated : %s (%s)", time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(jkey[1])),
                  jkey[0].split('/')[-1])
        if days_ago(jkey[1]) > args.age:
            # We've gone back past our cutoff
            log.info("Reached cutoff at timestamp %s", time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(jkey[1])))
            is_old = True
            if not args.keep_existing:
                break

        log.debug("gsutil cat %s 2> /dev/null" % (jkey[0]))
        process = subprocess.Popen("gsutil cat %s" % (jkey[0]), shell=True, stdout=subprocess.PIPE)
        process.wait()
        if process.returncode == 0:
            jdict = json.loads(process.stdout.read())
        else:
            log.debug('Unable to get %s' % (jkey[0]))

        if len(jdict.values()) != 1:
            raise SystemError('-listdir.json file should have '
                'a single key/value pair!')
        dirname = jdict.keys()[0]
        # fullpaths here since some files are in subdirectories
        fullpaths = [os.path.join(dirname, x) for x in jdict.values()[0]]
        keep_jkey = False # assume we are deleting the jkey
        for x in fullpaths:
            keep_file = True # assume we are keeping the file - this is the default behavior
            # if we are checking if we should keep existing files
            if args.keep_existing and is_old and not os.path.isfile(x):
                # don't keep the file if it is old and it does not exist
                keep_file = False

            keep_jkey = keep_jkey or keep_file  # keep the jkey if we are keeping any of the files
            key_to_keep = 'gs://%s/%s%s' % (args.bucket, args.name, x)
            # if we are keeping the file
            if keep_file and key_to_keep in to_delete:
                to_delete.remove(key_to_keep)

        if keep_jkey and jkey[0] in to_delete:
            to_delete.remove(jkey[0])

    to_del_len = len(to_delete)

    if args.list_deletes:
        # chunk into lines with 10 files each to avoid either too many lines
        # or one gigantic line
        to_del_list = list(to_delete)
        for delchunk in [','.join(to_del_list[i:i+10]) for \
                         i in xrange(0, to_del_len, 10)]:
            log.info("Files to delete: %s" % delchunk)

    if args.debug:
        log.debug("%s non-keeper keys to delete", to_del_len)
        ddates = list(set([time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(x[1])) for x in key_list \
                           if x[0] in to_delete]))
        ddates.sort()
        log.debug("deletion dates : %s", ddates)
        log.debug("Test mode, nothing deleted")
        return

    if to_del_len > 0:
        try:
            log.info("Deleting %s keys", to_del_len)
            
            # Remove the -m option to get the debug output from gsutil
            process = subprocess.Popen(['gsutil','-m','rm','-I'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
            
            for key in to_delete:
                process.stdin.write("%s\n" % (key))
                log.debug("Deleting key %s" %(key))

            output = process.communicate()[0]

            if process.returncode != 0:
                raise Exception("Unable to delete keys" % (output))

        except Exception as e:
            log.error('GS delete ERR, will try again later [%s]', e)
            return

        log.info("%d keys deleted" % (to_del_len))

def main(log):
    parser = argparse.ArgumentParser(
        description='Clean SSTables from GS. Scroll backwards through '
        '-listdir.json keys in chronological order collecting a "keeper" '
        'list until it reaches it\'s age cutoff. Deletes all keys not in that '
        'list')
    parser.add_argument(
        '-d',
        '--debug',
        dest='debug',
        action='store_true',
        help='Run in debug mode, will not delete keys. Implies -v')
    parser.add_argument(
        '-l',
        '--list-deletes',
        dest='list_deletes',
        action='store_true',
        help='Log every file being deleted')
    parser.add_argument(
        '-e',
        '--keep-existing',
        dest='keep_existing',
        action='store_true',
        help='Keep files that still exist on this server regardless of age')
    parser.add_argument(
        '-n',
        '--name',
        dest='name',
        required=False,
        help='Use this name instead of the FQDN to identify the files from '
             'this host')
    parser.add_argument(
        'bucket',
        help='GS Bucket')
    parser.add_argument(
        'path',
        help='Path portion of key in GS')
    parser.add_argument(
        'age',
        type=int,
        help='How many days worth of backups to keep')
    args = parser.parse_args()
    clean_backups(args, log)

if __name__ == '__main__':

    log = logging.getLogger('tablechop')
    stderr = logging.StreamHandler()
    stderr.setFormatter(logging.Formatter(
        '%(name)s [%(asctime)s] %(levelname)s %(message)s'))
    log.addHandler(stderr)
    if os.environ.get('TDEBUG', False):
        log.setLevel(logging.DEBUG)
    else:
        log.setLevel(logging.INFO)

    main(log)
